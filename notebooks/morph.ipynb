{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Sphere morphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interact \n",
    "import gc\n",
    "import open3d as o3d\n",
    "\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n",
    "from src.morph import *\n",
    "\n",
    "random.seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sphere morphing\n",
    "\n",
    "visualise the morphing of a sphere into a shape as an animation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "# visualise the morphing of a sphere into a shape\n",
    "cld1_name = \"data/plane1.pcd\"\n",
    "visualise = True\n",
    "#loss_funcs = [ \"chamfer\", \"emd\", \"uniform\", \"reverse\", \"single\"]\n",
    "#loss_funcs = [ \"uniform\", \"single\"]\n",
    "loss_funcs = [ \"density\"]\n",
    "for loss_func in loss_funcs:\n",
    "    print(loss_func)\n",
    "    run_morph(cld1_name, loss_func)\n",
    "    if visualise:\n",
    "        with open(\"data/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "            morphed = pickle.load(f)\n",
    "        colours = visualise_density(morphed, 'plasma_r')\n",
    "        with open(\"data/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "            pickle.dump(colours, f)\n",
    "        #create_point_cloud_animation(cloud_list, loss_func)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def view_density(loss_func): \n",
    "    with open(\"data/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        morphed = pickle.load(f)\n",
    "    with open(\"data/\" + loss_func + \"_dens.pkl\", \"rb\") as f:\n",
    "        colours = pickle.load(f)\n",
    "\n",
    "    create_point_cloud_animation(morphed, loss_func, True, colours[:,:,:3])\n",
    "    \n",
    "interact(view_density, loss_func=[ \"density\", \"uniform\", \"infocd\", \"single\", \"chamfer\", \"reverse\", \"emd\", \"direct\"]); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# visualise animation\n",
    "loss_func = \"emd\"\n",
    "with open(\"data/\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    morphed = pickle.load(f)\n",
    "print(morphed.shape, loss_func)\n",
    "cloud_list = [m for m in morphed]\n",
    "#create_point_cloud_animation(cloud_list, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = visualise_density(morphed, 'plasma_r')\n",
    "with open(\"data/\" + loss_func + \"_dens.pkl\", \"wb\") as f:\n",
    "    pickle.dump(colours, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch optimisation\n",
    "\n",
    "Perform sphere morphing in batches on entire PCN testset to generate output metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample\n",
    "shapenet_path = \"../experiments/ICCV2023-HyperCD/ShapeNetCompletion/test/complete/\"\n",
    "downsampled_path = \"../experiments/ICCV2023-HyperCD/ShapeNetCompletion/downsample/\"\n",
    "downsample = False\n",
    "if downsample:\n",
    "    folders = os.listdir(shapenet_path)\n",
    "    \n",
    "\n",
    "    for fl in tqdm(folders):\n",
    "        if not os.path.exists(downsampled_path+fl):\n",
    "            os.mkdir(downsampled_path+fl)\n",
    "            \n",
    "        files = os.listdir(shapenet_path + fl)\n",
    "        cloud =  o3d.geometry.PointCloud()\n",
    "        for cl in files:\n",
    "            points = np.array(o3d.io.read_point_cloud(shapenet_path + fl + \"/\" + cl).points)\n",
    "            choices = np.random.choice(len(points), 4096)\n",
    "            points = points[choices]\n",
    "            cloud.points = o3d.utility.Vector3dVector(points)\n",
    "            o3d.io.write_point_cloud(downsampled_path+fl + \"/\" + cl, cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate metrics and return cd and emd values only\n",
    "loss_func = \"uniform\"\n",
    "cd, emd = sphere_morph_metrics(loss_func, downsampled_path, save=False)\n",
    "print(cd[-1], emd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate values and save cd, emd, and point assignments\n",
    "loss_func = \"uniform\"\n",
    "\n",
    "sphere_morph_metrics(loss_func, downsampled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses on same axis\n",
    "def plot_losses(losses, labels, title):\n",
    "    x = np.arange(0, len(losses[0]))\n",
    "    plt.figure(figsize=(30, 6))\n",
    "    for i, loss in enumerate(losses):\n",
    "        plt.plot(x, loss, label=labels[i])\n",
    "\n",
    "    plt.xlabel(\"point cloud index\")\n",
    "    plt.ylabel(\"distance\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots\n",
    "loss_funcs = [\"chamfer\",]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"data/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, assignments = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n",
    "\n",
    "plot_losses(chamfer_list, loss_funcs, \"chamfer\")\n",
    "\n",
    "plot_losses(emd_list, loss_funcs, \"EMD\")\n",
    "\n",
    "print(emd_list[-1], chamfer_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"data/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "\n",
    "plot_losses(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency\n",
    "\n",
    "Consistency measurements for a single cloud. Refer to loss1.ipynb for batch metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "def measure_assignment_consistency(assignment, emd=None):\n",
    "    reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "    expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "    consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "    print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "    if emd is not None:\n",
    "        #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "        emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "        #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
