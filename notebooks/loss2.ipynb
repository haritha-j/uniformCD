{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Loss function visualisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from chamferdist import ChamferDistance\n",
    "import open3d as o3d\n",
    "\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains additional experiments on loss functions.\n",
    "\n",
    "Specifically it contains;\n",
    "\n",
    "1. Visualising point-wise distances from a loss function\n",
    "2. visualising results from point cloud reconstruction and point cloud completion methods\n",
    "\n",
    "#### data loading and pre-processing\n",
    "\n",
    "All data from sphere morphing must be loaded for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visusalise correpsondences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visaulise losses\n",
    "\n",
    "# load clouds\n",
    "cld1_name = \"data/24102.pcd\"\n",
    "cld2_name = \"data/24106.pcd\"\n",
    "blueprint = \"data/sample.ifc\"\n",
    "cuda = torch.device('cuda:0')\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure pairing loss\n",
    "src = torch.Tensor([cld1, cld1])\n",
    "tgt = torch.Tensor([cld2, cld2])\n",
    "\n",
    "chamferDist = ChamferDistance()\n",
    "nn = chamferDist(\n",
    "    src, tgt, bidirectional=True, return_nn=True, k=1\n",
    ")\n",
    "dist = torch.sum(nn[0].dists) + torch.sum(nn[1].dists)\n",
    "\n",
    "# compute the cyclical index (closest point of closest point). this should ideally be 0->n_points in order\n",
    "perfect_idx = torch.range(0,nn[0].idx.shape[1]-1, dtype=int) # 0-> N_points\n",
    "perfect_idx = perfect_idx[:,None] # add extra dimension\n",
    "perfect_idx = perfect_idx.repeat(nn[0].idx.shape[0], 1, 1) # batch_size\n",
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx) # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx) # tgt[[src[match]]]\n",
    "pair_loss = torch.sum(perfect_idx == true_idx_fwd)\n",
    "\n",
    "#print(nn[0].knn.shape, true_idx.shape, torch.flatten(true_idx[0]).shape)\n",
    "#pairs = torch.gather(nn[1].knn, 1, true_idx)\n",
    "paired_points_fwd = torch.stack([nn[0].knn[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "#paired_points_fwd = torch.stack([nn[1].knn[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "#paired_points_fwd = torch.stack([tgt[i][torch.flatten(true_idx_fwd[i])] for i in range(true_idx_fwd.shape[0])])\n",
    "paired_points_fwd = paired_points_fwd.reshape((paired_points_fwd.shape[0], \n",
    "                                   paired_points_fwd.shape[1], \n",
    "                                   paired_points_fwd.shape[3])) \n",
    "pair_dist_fwd = torch.sum(torch.square(paired_points_fwd - tgt))\n",
    "\n",
    "\n",
    "print(\"DS\", true_idx_bwd.shape, nn[0].knn.shape, src.shape, nn[0].idx.shape)\n",
    "paired_points_bwd = torch.stack([nn[1].knn[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])])\n",
    "#paired_points_bwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "print(paired_points_bwd.shape)\n",
    "#paired_points_bwd = torch.stack([nn[0].knn[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "paired_points_bwd = paired_points_bwd.reshape((paired_points_bwd.shape[0], \n",
    "                                   paired_points_bwd.shape[1], \n",
    "                                   paired_points_bwd.shape[3])) \n",
    "pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - src))\n",
    "\n",
    "print(\"pair\", true_idx_bwd[0].flatten().shape)\n",
    "print(dist, pair_loss, pair_dist_fwd, pair_dist_bwd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise cyclical pairs\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=true_idx_bwd[0].flatten(), loss=\"pair\", same_cloud=True)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_idx_fwd = torch.gather(nn[0].idx, 1, nn[1].idx) # tgt[[src[match]]]\n",
    "true_idx_bwd = torch.gather(nn[1].idx, 1, nn[0].idx) # tgt[[src[match]]]\n",
    "\n",
    "# manual chamfer loss\n",
    "paired_points_bwd = torch.stack([tgt[i][torch.flatten(nn[0].idx[i])] for i in range(nn[0].idx.shape[0])])\n",
    "pair_dist_bwd = paired_points_bwd - src\n",
    "#pair_dist_bwd = torch.sum(torch.square(paired_points_bwd - x))\n",
    "#paired_points_fwd = torch.stack([x[i][torch.flatten(nn[1].idx[i])] for i in range(nn[1].idx.shape[0])])\n",
    "paired_points_fwd = torch.stack([src[i][torch.flatten(true_idx_bwd[i])] for i in range(true_idx_bwd.shape[0])])\n",
    "pair_dist_fwd = paired_points_fwd - paired_points_bwd\n",
    "\n",
    "pair_dist = pair_dist_bwd + pair_dist_fwd\n",
    "pair_dist = torch.mul(torch.square(pair_dist), torch.square(pair_dist_bwd))\n",
    "pair_dist = torch.sum(pair_dist) \n",
    "print(pair_dist, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise EMD\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_emd(src, tgt, 0.05, 1000)\n",
    "\n",
    "ass = ass.detach().cpu().numpy()\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass.flatten(), loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise regular chamfer loss\n",
    "v = visualise_loss(cld1, cld2, blueprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise reverse weighted chamfer loss\n",
    "src = torch.Tensor([cld1]).cuda()\n",
    "tgt = torch.Tensor([cld2]).cuda()\n",
    "loss, ass = calc_reverse_weighted_cd_tensor(src, tgt, k=32, return_assignment=True)\n",
    "\n",
    "ass = ass[0].detach().cpu().numpy()\n",
    "\n",
    "unique = np.unique(ass)\n",
    "print(\"unique\", len(unique))\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, pairs=ass, loss=\"pair\", same_cloud=False)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load subsampled clouds\n",
    "cld1_name = \"data/24102s.pcd\"\n",
    "cld2_name = \"data/24103s.pcd\"\n",
    "\n",
    "cld1 = np.array(o3d.io.read_point_cloud(cld1_name).points)\n",
    "cld2 = np.array(o3d.io.read_point_cloud(cld2_name).points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise chamfer loss with coplanarity\n",
    "target_pcd_tensor = torch.tensor([cld2], device=cuda)\n",
    "src_pcd_tensor = torch.tensor([cld1], device=cuda)\n",
    "\n",
    "vect, dists = knn_vectors(src_pcd_tensor, target_pcd_tensor, 3)\n",
    "coplanarity = check_coplanarity(vect)\n",
    "coplanarity = coplanarity[0].detach().cpu().numpy()\n",
    "print(\"shapes\", coplanarity.shape, dists.shape)\n",
    "\n",
    "v = visualise_loss(cld1, cld2, blueprint, strength=coplanarity, k=3)\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualise cross section correspondence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_correspondences(a, b, ass):\n",
    "    points = np.vstack([a,b])\n",
    "    start_idx = np.arange(a.shape[0])\n",
    "    end_idx = ass + a.shape[0]\n",
    "    lines = np.stack([start_idx, end_idx], axis=1)\n",
    "    print(points.shape, lines.shape, lines[:3])\n",
    "    \n",
    "    colors = [[1, 0, 0] for i in range(len(lines))]\n",
    "    line_set = o3d.geometry.LineSet(\n",
    "        points=o3d.utility.Vector3dVector(points),\n",
    "        lines=o3d.utility.Vector2iVector(lines),\n",
    "    )\n",
    "    line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    return line_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_path = \"data/plane_slice2.pcd\"\n",
    "points = np.array(o3d.io.read_point_cloud(cloud_path).points)\n",
    "# flatten\n",
    "points[:,1] = 0\n",
    "gt = o3d.geometry.PointCloud()\n",
    "gt.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "# generate points with some random variations\n",
    "source = o3d.geometry.PointCloud()\n",
    "\n",
    "points2 = np.copy(points)\n",
    "variation = (np.random.rand(points2.shape[0], points2.shape[1]) - 0.5)/10\n",
    "variation[:,1] = 0\n",
    "points2 += variation\n",
    "source.points = o3d.utility.Vector3dVector(points2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "# morph a shape into a different shape\n",
    "# by optimising chamfer loss iteratively\n",
    "# total points = num_points**2\n",
    "def optimise_shape(src_pcd_tensor, tgt_pcd_tensor, iterations=5, learning_rate=0.01, loss_func= \"chamfer\", calc_emd= False):\n",
    "    \n",
    "    cuda = torch.device(\"cuda\")\n",
    "    \n",
    "    # optimise\n",
    "    optimizer = torch.optim.Adam([tgt_pcd_tensor], lr=learning_rate)\n",
    "    intermediate, losses, assingments = [], [], []\n",
    "    chamferDist = ChamferDistance()\n",
    "    assignments = []\n",
    "\n",
    "    for i in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_func == \"chamfer\":\n",
    "            nn = chamferDist(\n",
    "                src_pcd_tensor, tgt_pcd_tensor, bidirectional=True, return_nn=True)\n",
    "            loss = torch.sum(nn[1].dists) + torch.sum(nn[0].dists)\n",
    "            assignment = [nn[0].idx[:,:,0].detach().cpu().numpy(), nn[1].idx[:,:,0].detach().cpu().numpy()]\n",
    "        elif loss_func == \"emd\":\n",
    "            loss, assignment = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "            assignment = assignment.detach().cpu().numpy()\n",
    "        elif loss_func == \"uniform\":\n",
    "            loss, assignment = calc_uniform_chamfer_loss_tensor(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True, k=4)\n",
    "        elif loss_func == \"infocd\":\n",
    "            loss, assignment = calc_cd_like_InfoV2(src_pcd_tensor, tgt_pcd_tensor, return_assignment=True)\n",
    "        elif loss_func == \"direct\":\n",
    "            loss = calc_direct(src_pcd_tensor, tgt_pcd_tensor)\n",
    "        else:\n",
    "            print(\"unspecified loss\")\n",
    "            \n",
    "        #print(\"a\", assignment[0].shape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"iteration\", i, \"loss\", loss.item())\n",
    "        \n",
    "        intermediate.append(tgt_pcd_tensor.clone())\n",
    "\n",
    "    # calculate final chamfer loss\n",
    "    dist = chamferDist(src_pcd_tensor, tgt_pcd_tensor, bidirectional=True)\n",
    "    if calc_emd:\n",
    "        emd_loss, _ = calc_emd(tgt_pcd_tensor, src_pcd_tensor, 0.05, 50)\n",
    "        print(\"final chamfer dist\", dist.item(), \"emd\", emd_loss.item())\n",
    "    else:\n",
    "        print(\"final chamfer dist\", dist.item())\n",
    "    \n",
    "    # save assignments for analysis\n",
    "#     if measure_consistency:\n",
    "#         with open(\"data/assignments_\" + loss_func + \".pkl\", \"wb\") as f:\n",
    "#             pickle.dump(assingments, f)\n",
    "            \n",
    "    intermediate = torch.stack(intermediate)\n",
    "    return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create tensors\n",
    "cuda = torch.device(\"cuda\")\n",
    "x = torch.tensor([points], device=cuda)\n",
    "y = torch.tensor([points2], device=cuda, requires_grad=True)\n",
    "\n",
    "iterations = 50\n",
    "intermediate = optimise_shape(x, y, iterations=iterations, learning_rate=0.01, loss_func=\"chamfer\")\n",
    "print(intermediate.shape)\n",
    "intermediate = intermediate.reshape((iterations, x.shape[1], x.shape[2])).detach().cpu().numpy()\n",
    "print(intermediate.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_o3d_paths(a, stops):\n",
    "    steps = stops.shape[0]\n",
    "    print(stops.shape)\n",
    "    points = np.vstack([a]+[stops[i] for i in range(steps)])\n",
    "    line_sets = []\n",
    "    #print(points.shape)\n",
    "    for j in  range(steps):\n",
    "        if j ==0:\n",
    "            start_idx = np.arange(a.shape[0])\n",
    "        else:\n",
    "            start_idx = start_idx + a.shape[0]\n",
    "        end_idx = start_idx + a.shape[0]\n",
    "        lines = np.stack([start_idx, end_idx], axis=1)\n",
    "        #print(points.shape, lines.shape, lines[:3])\n",
    "\n",
    "        colors = [[1, 1-0.1*j, 0] for i in range(len(lines))]\n",
    "        line_set = o3d.geometry.LineSet(\n",
    "            points=o3d.utility.Vector3dVector(points),\n",
    "            lines=o3d.utility.Vector2iVector(lines),\n",
    "        )\n",
    "        line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "        \n",
    "        line_sets.append(line_set)\n",
    "    \n",
    "    return line_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = o3d.geometry.PointCloud()\n",
    "interm.points = o3d.utility.Vector3dVector(intermediate[0])\n",
    "\n",
    "end = o3d.geometry.PointCloud()\n",
    "end.points = o3d.utility.Vector3dVector(intermediate[-1])\n",
    "\n",
    "\n",
    "source.paint_uniform_color([0., 0.706, 1])\n",
    "end.paint_uniform_color([0., 0.706, 1])\n",
    "interm.paint_uniform_color([0.5, 0.5, 0])\n",
    "gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "#line_sets = draw_o3d_paths(points2, intermediate[1:])\n",
    "#o3d.visualization.draw_geometries([gt, source, interm] + line_sets)\n",
    "\n",
    "# start from the 2nd step onwards\n",
    "line_sets = draw_o3d_paths(points2, intermediate)\n",
    "o3d.visualization.draw_geometries([source, gt, end] + line_sets)\n",
    "#o3d.visualization.draw_geometries([source, gt] )\n",
    "#o3d.visualization.draw_geometries([end, gt] )\n",
    "#o3d.visualization.draw_geometries([interm, gt, end] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source.paint_uniform_color([0., 0.706, 1])\n",
    "# gt.paint_uniform_color([1., 0.706, 1])\n",
    "# gt.paint_uniform_color([1., 0.706, 1])\n",
    "\n",
    "# line_set = draw_o3d_correspondences(points2, points, assignment)\n",
    "# o3d.visualization.draw_geometries([gt, source, line_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise autoencoder results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise npy point cloud set\n",
    "\n",
    "# chamfer\n",
    "ch_savepath = \"../PointSWD/logs2/reconstruction/model/modelnet40/\"\n",
    "ch_inp_list = np.load(os.path.join(ch_savepath, \"input.npy\"))\n",
    "ch_rec_list = np.load(os.path.join(ch_savepath, \"reconstruction.npy\"))\n",
    "\n",
    "# new\n",
    "savepath = \"../PointSWD/logs24/reconstruction/model/modelnet40/\"\n",
    "inp_list = np.load(os.path.join(savepath, \"input.npy\"))\n",
    "rec_list = np.load(os.path.join(savepath, \"reconstruction.npy\"))\n",
    "\n",
    "print(rec_list.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ifc = setup_ifc_file(blueprint)\n",
    "limit = 10\n",
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(vis_ifc_and_cloud(ifc, [ch_inp_list[i].astype(\"float64\"), ch_rec_list[i].astype(\"float64\")]))\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis = []\n",
    "\n",
    "for i in range(limit):\n",
    "    vis.append(vis_ifc_and_cloud(ifc, [inp_list[i].astype(\"float64\"), rec_list[i].astype(\"float64\")]))\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare losses\n",
    "def evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, batch_size=None):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    if batch_size == None:\n",
    "        batch_size = len(inp_list)\n",
    "        \n",
    "    # create empty dict\n",
    "    all_losses = {func:[] for func in loss_funcs}\n",
    "    \n",
    "    # split into batches\n",
    "    for i in tqdm(range(math.ceil(len(inp_list)/batch_size))):\n",
    "        a = i*batch_size\n",
    "        b = min(a+batch_size, len(inp_list))\n",
    "        #print(a,b)\n",
    "        \n",
    "        # compute losses\n",
    "        inp_tensor = torch.tensor(inp_list[a:b], device=cuda)\n",
    "        rec_tensor = torch.tensor(rec_list[a:b], device=cuda)\n",
    "        losses = calculate_3d_loss(inp_tensor, rec_tensor, loss_funcs)\n",
    "        \n",
    "        # sum losses\n",
    "        for k, v in losses.items():\n",
    "            all_losses[k].append(v)\n",
    "            \n",
    "    # average losses\n",
    "    for k in all_losses:\n",
    "        all_losses[k] = np.average(np.array(all_losses[k]))\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "loss_funcs = [\"chamfer\", \"reverse\", \"emd\"]\n",
    "\n",
    "# chamfer\n",
    "ch_loss = evaluate_autoencoder_results(ch_inp_list, ch_rec_list, loss_funcs, 512)\n",
    "\n",
    "# new\n",
    "loss = evaluate_autoencoder_results(inp_list, rec_list, loss_funcs, 512)\n",
    "\n",
    "print(\"chamfer tuned\", ch_loss)\n",
    "print(\"new\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate completion results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DCD (VRC, MVP) / PointAttn (PCN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce density colourmaps that are normalised with their pairs\n",
    "\n",
    "# get densities\n",
    "def get_density(clouds):\n",
    "    # compute nearest neighbours to calculated density\n",
    "    clouds = torch.tensor(clouds, device=\"cuda\")\n",
    "    chamferDist = ChamferDistance()\n",
    "    nn = chamferDist(clouds, clouds, bidirectional=False, return_nn=True, k=32)\n",
    "    \n",
    "    density = torch.mean(nn[0].dists[:,:,1:], dim=2)\n",
    "    eps = 0.00001\n",
    "    density = 1 / (density + eps)\n",
    "    return density\n",
    "\n",
    "\n",
    "# normalise for each example across prediction sets\n",
    "def normalise_densities(density_sets):\n",
    "    densities = torch.stack(density_sets)\n",
    "    highs, lows = torch.max(densities, 2).values, torch.min(densities, 2).values\n",
    "    highs, lows = torch.max(highs, 0).values, torch.min(lows, 0).values\n",
    "    #print(densities.shape, highs.shape)\n",
    "    \n",
    "    #highs = torch.reshape(highs, densities.shape)\n",
    "    highs = highs.unsqueeze(0).unsqueeze(-1)\n",
    "    highs = highs.expand(densities.shape[0], densities.shape[1], densities.shape[2])\n",
    "    lows = lows.unsqueeze(0).unsqueeze(-1)\n",
    "    lows = lows.expand(densities.shape[0], densities.shape[1], densities.shape[2])\n",
    "    diff = highs - lows\n",
    "    densities = (densities - lows) / diff\n",
    "    \n",
    "    return densities[0], densities[1], densities[2], densities[3]\n",
    "    \n",
    "\n",
    "# represent density with colour and combine with point cloud\n",
    "def get_coloured_clouds(clouds, density, colormap_name='plasma_r'):\n",
    "    density = density.detach().cpu().numpy()\n",
    "    colours = np.zeros((density.shape[0], density.shape[1], 4))\n",
    "    colormap = plt.get_cmap(colormap_name)\n",
    "    \n",
    "    for i, cloud in enumerate(density):\n",
    "        for j, pt in enumerate(cloud):\n",
    "            colours[i,j] = colormap(pt)\n",
    "            \n",
    "#     clouds = clouds.detach().cpu().numpy()\n",
    "    colours = colours[:,:,:3]\n",
    "    pcds = []\n",
    "    \n",
    "    for i, cl in enumerate(clouds):\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(cl)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colours[i])\n",
    "        pcds.append(pcd)\n",
    "    \n",
    "    return pcds, colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cloud_list_vrc(path, prefix):\n",
    "    limit = 20\n",
    "    cloud_sets = []\n",
    "    \n",
    "    for i in range(limit):\n",
    "        f_name = \"fine\"+str(i)+\".pkl\"\n",
    "        with open(path + f_name, \"rb\") as f:\n",
    "            pred, partial, gt = pickle.load(f)\n",
    "            if prefix == \"pred\":\n",
    "                clouds = pred\n",
    "            elif prefix == \"gt\":\n",
    "                clouds = gt\n",
    "            else:\n",
    "                clouds = partial\n",
    "        cloud_sets.append(clouds.detach().cpu().numpy())\n",
    "    cloud_sets = np.vstack(cloud_sets)\n",
    "    print(cloud_sets.shape)\n",
    "    \n",
    "    return cloud_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"pointattn\"\n",
    "if model == \"VRC\":\n",
    "    uniform_path = \"Density_aware_Chamfer_Distance/outputs_uniform/\"\n",
    "    dcd_path = \"Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "\n",
    "elif model == \"pointattn\":\n",
    "    uniform_path = \"../PointAttN/outputs/\"\n",
    "    dcd_path = \"../PointAttN/outputs_cd/\"\n",
    "\n",
    "cld_uniform = get_cloud_list_vrc(uniform_path, \"pred\")\n",
    "cld_dcd = get_cloud_list_vrc(dcd_path, \"pred\")\n",
    "cld_gt = get_cloud_list_vrc(dcd_path, \"gt\")\n",
    "cld_partial = get_cloud_list_vrc(dcd_path, \"partial\")\n",
    "\n",
    "d_uniform = get_density(cld_uniform)\n",
    "d_dcd = get_density(cld_dcd)\n",
    "d_gt = get_density(cld_gt)\n",
    "d_partial = get_density(cld_partial)\n",
    "\n",
    "print(d_partial.shape)\n",
    "\n",
    "# produce density colourmaps that are normalised with their pairs\n",
    "d_uniform, d_dcd, d_gt, d_partial = normalise_densities([d_uniform, d_dcd, d_gt, d_partial])\n",
    "\n",
    "v_uniform, _ = get_coloured_clouds(cld_uniform, d_uniform)\n",
    "v_dcd, _ = get_coloured_clouds(cld_dcd, d_dcd)\n",
    "v_gt, col = get_coloured_clouds(cld_gt, d_gt)\n",
    "v_partial, _ = get_coloured_clouds(cld_partial, d_partial)\n",
    "# c_dcd = get_colours(d_dcd)\n",
    "# c_gt = get_colours(d_gt)\n",
    "# c_partial = get_colours(d_partial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_side_by_side(v1, v2, v3, v4, i):\n",
    "    # shift points\n",
    "    cl1, cl2, cl3, cl4 = v1[i], v2[i], v3[i], v4[i]\n",
    "    cl1.points = o3d.utility.Vector3dVector(np.array(cl1.points) - np.array([1,0,0]))\n",
    "    cl3.points = o3d.utility.Vector3dVector(np.array(cl3.points) + np.array([1,0,0]))\n",
    "    cl4.points = o3d.utility.Vector3dVector(np.array(cl4.points) + np.array([2,0,0]))\n",
    "    \n",
    "    o3d.visualization.draw_geometries([cl1, cl2, cl3, cl4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(v_uniform))\n",
    "cloud_list = [1,23, 34, 18, 36, 45]\n",
    "for i in range(0,50):\n",
    "#for i in cloud_list:\n",
    "    print(i)\n",
    "    view_side_by_side(v_gt, v_dcd, v_uniform, v_partial, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a desired set of clouds\n",
    "def save_cloud(clouds,  name, i, colours=None):\n",
    "    directory = \"mitsuba/\"\n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     pcd.points = o3d.utility.Vector3dVector(clouds[i])\n",
    "#     o3d.io.write_point_cloud(directory + name + str(i) + \".ply\", pcd)\n",
    "    with open(directory + name + str(i) + \".npy\", \"wb\") as f:\n",
    "        np.save(f, clouds[i])\n",
    "    if colours is not None:\n",
    "        with open(directory + name + str(i) + \"c_.npy\", \"wb\") as f:\n",
    "            np.save(f, colours[i])\n",
    "    \n",
    "i = 4\n",
    "# save_cloud(cld_uniform, \"uniform\", i)\n",
    "# save_cloud(cld_dcd, \"dcd\", i)\n",
    "# save_cloud(cld_partial, \"partial\", i)\n",
    "# save_cloud(cld_gt, \"gt\", i)\n",
    "\n",
    "for i in cloud_list:\n",
    "    save_cloud(cld_gt, \"gt\", i+1)\n",
    "    save_cloud(cld_uniform, \"bal\", i+1)\n",
    "    save_cloud(cld_dcd, \"dcd\", i+1)\n",
    "    save_cloud(cld_partial, \"part\", i+1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cloud_list_vcn(path, prefix, ifc, col=0):\n",
    "    limit = 2\n",
    "    vis = []\n",
    "    \n",
    "    for i in range(limit):\n",
    "        f_name = \"fine\"+str(i)+\".pkl\"\n",
    "        with open(path + f_name, \"rb\") as f:\n",
    "            pred, partial, gt = pickle.load(f)\n",
    "            if prefix == \"pred\":\n",
    "                clouds = pred\n",
    "            elif prefix == \"gt\":\n",
    "                clouds = gt\n",
    "            else:\n",
    "                clouds = partial\n",
    "        for cl in clouds:\n",
    "            cloud_list = [None, None, None]\n",
    "            cloud_list[col] = cl.detach().cpu().numpy().astype(np.double)\n",
    "            vis.append(vis_ifc_and_cloud(ifc, cloud_list))\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_path = \"Density_aware_Chamfer_Distance/outputs_uniform/\"\n",
    "dcd_path = \"Density_aware_Chamfer_Distance/outputs_dcd/\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "v_uniform = view_cloud_list_vcn(uniform_path, \"pred\", ifc, 0)\n",
    "v_dcd = view_cloud_list_vcn(dcd_path, \"pred\", ifc, 1)\n",
    "v_gt = view_cloud_list_vcn(dcd_path, \"gt\", ifc, 2)\n",
    "v_partial = view_cloud_list_vcn(dcd_path, \"partial\", ifc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(v_uniform))\n",
    "for i in range(len(v_uniform)):\n",
    "    print(v_uniform[i], v_dcd[i], v_gt[i], v_partial[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InfoCD (Seedformer, PCN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "\n",
    "# load NPYs from each. (GT, Complete, CD, InfoCD, UniformCD)\n",
    "category_id = \"02933112/\"\n",
    "cd_path = \"../ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_cd/\"\n",
    "info_path = \"../ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_22_14_49_07/outputs_info/\"\n",
    "uniform_path = \"../ICCV2023-HyperCD/pcn/train_pcn_Log_2024_02_29_23_43_45/outputs_uniform/\"\n",
    "\n",
    "blueprint = \"data/sample.ifc\"\n",
    "ifc = setup_ifc_file(blueprint)\n",
    "\n",
    "def view_cloud_list_seedformer(category_id, path, prefix, ifc, col=0):\n",
    "    files = os.listdir(os.path.join(path,category_id))\n",
    "    files_filtered = [f for f in files if prefix in f]\n",
    "    files_filtered.sort()\n",
    "    #print(len(files), len(files_filtered))\n",
    "    \n",
    "    limit = 10\n",
    "    vis = []\n",
    "    count = 0\n",
    "    \n",
    "    for f in files_filtered:\n",
    "        count +=1\n",
    "        if count == limit:\n",
    "            break\n",
    "        cloud = np.load(os.path.join(path, category_id, f))\n",
    "        cloud_list = [None, None, None]\n",
    "        cloud_list[col] = cloud.astype(\"float64\")\n",
    "        vis.append(vis_ifc_and_cloud(ifc, cloud_list))\n",
    "    return vis\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_cd = view_cloud_list_seedformer(category_id, cd_path, \"pred\", ifc, 0)\n",
    "v_info = view_cloud_list_seedformer(category_id, info_path, \"pred\", ifc, 1)\n",
    "v_uniform = view_cloud_list_seedformer(category_id, uniform_path, \"pred\", ifc, 2)\n",
    "v_complete = view_cloud_list_seedformer(category_id, cd_path, \"gt\", ifc, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(v_cd)):\n",
    "    print(v_cd[i], v_info[i], v_uniform[i], v_complete[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
