{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpvL68OfBEQC"
   },
   "source": [
    "# Loss Function Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJ47VNF7fmTS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import open3d as o3d\n",
    "import scipy.stats\n",
    "\n",
    "# from src.elements import *\n",
    "# from src.ifc import *\n",
    "# from src.preparation import *\n",
    "from src.visualisation import *\n",
    "from src.chamfer import *\n",
    "from src.morph import *\n",
    "\n",
    "random.seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains experiments on loss functions.\n",
    "\n",
    "Specifically it contains;\n",
    "\n",
    "1. Analysis of results from sphere morphing, including consistency metrics\n",
    "2. Visualising point-wise distances from a loss function\n",
    "3. Measuring loss correlation in results from completion models\n",
    "4. Visualising loss curves for model training\n",
    "\n",
    "#### data loading and pre-processing\n",
    "\n",
    "All data from sphere morphing must be loaded for analysis. The morph notebook must be run prior to this section in order to generate results from sphere morphing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper code to recombine batched output from uniformCD\n",
    "file_prefix = \"data/uniform_metrics\"\n",
    "icd_chamfer_list, icd_emd_list, icd_assignment_list = [], [], []\n",
    "\n",
    "for i in range(1,9):\n",
    "    with open(file_prefix + str(i) + \".pkl\", \"rb\") as f:\n",
    "        chamfer, emd, ass = pickle.load(f)\n",
    "        print(len(ass), len(ass[0]), len(ass[0][0]), ass[0][0][0].shape)\n",
    "        icd_chamfer_list.append(chamfer)\n",
    "        icd_emd_list.append(emd)\n",
    "        icd_assignment_list.append(ass[0])\n",
    "\n",
    "icd_assignment_list = np.array(icd_assignment_list)        \n",
    "icd_assignment_list = np.transpose(icd_assignment_list, axes=(1,2,0,3,4))\n",
    "icd_assignment_list = np.reshape(icd_assignment_list, (101, 2, 8*150, 4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_cd = np.sum(np.array(icd_chamfer_list), axis=0)\n",
    "uniform_emd = np.sum(np.array(icd_emd_list), axis=0)\n",
    "print(uniform_emd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "loss_funcs = [\"emd\"]\n",
    "#loss_funcs = [\"infocd\", \"chamfer\", \"emd\", \"uniform\"]\n",
    "chamfer_list, emd_list = [], []\n",
    "for loss_func in loss_funcs:\n",
    "    if loss_func == \"uniform\":\n",
    "        continue\n",
    "    with open(\"data/\" + loss_func + \"_metrics.pkl\", \"rb\") as f:\n",
    "        chamfer, emd, ass = pickle.load(f)\n",
    "        chamfer_list.append(chamfer)\n",
    "        emd_list.append(emd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"uniform\" in loss_funcs:\n",
    "    chamfer_list.append(uniform_cd)\n",
    "    emd_list.append(uniform_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) \n",
    "loss_funcs = [\"infoCD\", \"CD\", \"EMD\", \"UniformCD\"]\n",
    "\n",
    "plot_dists(axes[1], chamfer_list, loss_funcs, \"Chamfer Distance\", \"iterations\", log=True)\n",
    "plot_dists(axes[0], emd_list, loss_funcs, \"Earth Mover's Distance\", \"iterations\", log=True)\n",
    "print(\"EMD infocd\", emd_list[0][-1], \"chamfer\", emd_list[1][-1],  \"emd\", emd_list[2][-1], \"uniform\", emd_list[3][-1])\n",
    "print(\"CD infocd\", chamfer_list[0][-1], \"chamfer\", chamfer_list[1][-1], \"emd\", chamfer_list[2][-1], \"uniform\", chamfer_list[3][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ass), len(ass[0]), ass[0][0].shape, len(ass[0][0]), ass[0][0][0].shape)\n",
    "print(emd_list[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load losses\n",
    "# single, not batch\n",
    "loss_types = [\"reverse\", \"chamfer\", \"emd\", \"pair\"]\n",
    "losses = []\n",
    "\n",
    "for loss_func in loss_types:\n",
    "    with open(\"data/loss_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "        losses.append(pickle.load(f))\n",
    "        \n",
    "plot_dists(losses, loss_types, \"loss function comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consistency metrics\n",
    "\n",
    "We measure;\n",
    "\n",
    "1. backward assignment consistency\n",
    "2. coorrespondence coverage\n",
    "3. correspondence stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency between forward and backward correspondences for chamfer distance\n",
    "# optionally compare against the ideal assignment, as measured by EMD\n",
    "def measure_assignment_consistency(assignment, emd=None):\n",
    "    reverse_assignment = torch.gather(assignment[0], 0, assignment[1])\n",
    "    expected = torch.arange(assignment[0].shape[0], device=torch.device(\"cuda\"))\n",
    "    consistency = torch.sum(torch.eq(expected, reverse_assignment).long())\n",
    "    print(\"consistency\", consistency.item(), len(torch.unique(assignment[0])), len(torch.unique(assignment[1])))\n",
    "    \n",
    "    if emd is not None:\n",
    "        #print(emd[:5], assignment[0][:5], assignment[1][:5])\n",
    "        emd_consistency = torch.sum(torch.eq(emd, assignment[0]).long())\n",
    "        #print(\"emd_consistency\", emd_consistency.item(), len(torch.unique(emd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure conssistency for EMD approx correspondences\n",
    "# this is different from other distances as only a one way assignment is returned\n",
    "def measure_batch_assignment_consistency_emd(assignment):\n",
    "    consistencies = np.array([4096 for i in range(assignment.shape[-1])])\n",
    "    cuda = torch.device(\"cuda\")\n",
    "\n",
    "    uniques = []\n",
    "    changes = []\n",
    "    for i in tqdm(range(len(assignment))):\n",
    "        class_assignment = torch.tensor(assignment[i], device=cuda)\n",
    "        \n",
    "        unique = 0\n",
    "        for j in range(len(class_assignment)):\n",
    "            unique += len(torch.unique(class_assignment[j]))\n",
    "        unique = (unique/len(class_assignment))\n",
    "        uniques.append(unique)\n",
    "        \n",
    "        # check rate of change of matches\n",
    "        if i==0:\n",
    "            change = 0\n",
    "        else:\n",
    "            change = (assignment[i] == assignment[i-1]).sum()\n",
    "            change = (change/len(class_assignment[0]))\n",
    "        changes.append(change)\n",
    "        \n",
    "        print(\"un\", unique, \"change\", change)\n",
    "        \n",
    "    # save results\n",
    "    with open(\"data/\" + \"emd\" + \"_consistency.pkl\", \"wb\") as f:\n",
    "        pickle.dump([consistencies, uniques, changes], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "measure_batch_assignment_consistency_emd(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure consistency between forward and backward correspondences\n",
    "# measure number of unique assignments\n",
    "# measure rate of change of assignments\n",
    "def measure_batch_assignment_consistency(assignment, loss):\n",
    "    cuda = torch.device(\"cuda\")\n",
    "    # loop through iterations\n",
    "    consistencies = []\n",
    "    uniques = []\n",
    "    changes = []\n",
    "    for i in tqdm(range(len(assignment))):\n",
    "        \n",
    "        # check reverse consistency\n",
    "        class_assignment = torch.tensor(assignment[i], device=cuda)\n",
    "        reverse_assignment = torch.gather(class_assignment[0], 1, class_assignment[1])\n",
    "        #print(\"r\", reverse_assignment.shape)\n",
    "        expected = torch.arange(class_assignment[0].shape[1], device=cuda)\n",
    "        expected = expected.repeat(class_assignment[0].shape[0], 1)\n",
    "        #print(\"e\", expected.shape)\n",
    "        consistency = torch.sum(torch.eq(expected, reverse_assignment).long())/len(class_assignment[0])\n",
    "        consistencies.append(consistency.item())\n",
    "        \n",
    "        # check uniqueness of matches\n",
    "        unique = 0\n",
    "        for j in range(len(class_assignment[0])):\n",
    "            unique += (len(torch.unique(class_assignment[0][j])) +\n",
    "                       len(torch.unique(class_assignment[1][j])))\n",
    "        unique = (unique/len(class_assignment[0]))/2\n",
    "        uniques.append(unique)\n",
    "        \n",
    "        # check rate of change of matches\n",
    "        if i==0:\n",
    "            change = 0\n",
    "        else:\n",
    "            change = (assignment[i] == assignment[i-1]).sum()\n",
    "            change = (change/len(class_assignment[0]))/2\n",
    "        changes.append(change)\n",
    "        \n",
    "        print(\"un\", unique, \"consistency\", consistency.item(), \"change\", change)\n",
    "\n",
    "    \n",
    "    # save results\n",
    "    with open(\"data/\" + loss + \"_consistency.pkl\", \"wb\") as f:\n",
    "        pickle.dump([consistencies, uniques, changes], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "loss = \"emd\"\n",
    "#ass = np.array(ass)\n",
    "\n",
    "if loss == \"infocd\":\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4,5))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    \n",
    "if loss == \"chamfer\":\n",
    "    ass = np.transpose(ass, axes=(1,2,0,3,4))\n",
    "    ass = np.reshape(ass, (101, 2, 8*150, 4096))\n",
    "    \n",
    "if loss == \"emd\":\n",
    "    ass = np.array(ass)\n",
    "    ass = np.transpose(ass, axes=(1,0,2,3))\n",
    "    ass = np.reshape(ass, (101, 8*150, 4096))\n",
    "print(ass.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if loss == \"uniform\":\n",
    "    measure_batch_assignment_consistency(icd_assignment_list, loss=\"UniformCD\")\n",
    "else:\n",
    "    measure_batch_assignment_consistency(ass, loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot consistency metrics\n",
    "\n",
    "loss_funcs = [\"infocd\", \"chamfer\", \"emd\", \"UniformCD\"]\n",
    "unique_list, consistency_list, change_list = [], [], []\n",
    "for loss_func in loss_funcs:\n",
    "    with open(\"data/\" + loss_func + \"_consistency.pkl\", \"rb\") as f:\n",
    "        consistency, unique, change = pickle.load(f)\n",
    "        consistency_list.append(consistency)\n",
    "        unique_list.append(unique)\n",
    "        change_list.append(change)\n",
    "print(change_list[0][-1], change_list[1][-1])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 5))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=14) \n",
    "plt.rc('ytick', labelsize=14) \n",
    "\n",
    "limit= 50\n",
    "loss_funcs = [\"InfoCD\", \"CD\", \"EMD\", \"UniformCD\"]\n",
    "\n",
    "plot_dists(axes[0], consistency_list, loss_funcs, title=\"Backward consistency\", \n",
    "           ylabel=\"no. of points\", xlabel=\"Iterations\", log=False, limit=limit, legend=False)\n",
    "plot_dists(axes[1], unique_list, loss_funcs, title=\"Point coverage\", \n",
    "           ylabel=\"\", xlabel=\"Iterations\", log=False, limit=limit, legend=False)\n",
    "plot_dists(axes[2], change_list, loss_funcs, title=\"Correspondence variation\", \n",
    "           ylabel=\"\", xlabel=\"Iterations\", log=False, limit=limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare correspondences\n",
    "#TODO: include consistency in top5 matches?\n",
    "loss_func = \"emd\"\n",
    "with open(\"data/assignments_\" + \"emd\" + \".pkl\", \"rb\") as f:\n",
    "    emd_assignment = pickle.load(f)\n",
    "\n",
    "with open(\"data/assignments_\" + loss_func + \".pkl\", \"rb\") as f:\n",
    "    assignment = pickle.load(f)\n",
    "    \n",
    "for i, ass in enumerate(assignment):\n",
    "    #measure_assignment_consistency(ass, emd_assignment[i][0])\n",
    "    measure_assignment_consistency(ass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-wise correspondence distance visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clouds\n",
    "tgt_path = \"data/target.pcd\"\n",
    "l1_path = \"data/l1d.pcd\"\n",
    "l2_path = \"data/l2d.pcd\"\n",
    "l3_path = \"data/l3d.pcd\"\n",
    "l4_path = \"data/l4d.pcd\"\n",
    "\n",
    "tgt = np.array(o3d.io.read_point_cloud(tgt_path).points)\n",
    "l1 = np.array(o3d.io.read_point_cloud(l1_path).points)\n",
    "l2 = np.array(o3d.io.read_point_cloud(l2_path).points)\n",
    "l3 = np.array(o3d.io.read_point_cloud(l3_path).points)\n",
    "l4 = np.array(o3d.io.read_point_cloud(l4_path).points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "def get_point_weights(tgt, l1, l2, l3, l4, loss_func=\"chamfer\"):\n",
    "    w1, loss1 = get_point_distance(tgt, l1, loss_func)\n",
    "    w2, loss2 = get_point_distance(tgt, l2, loss_func)\n",
    "    w3, loss3 = get_point_distance(tgt, l3, loss_func)\n",
    "    w4, loss4 = get_point_distance(tgt, l4, loss_func)\n",
    "\n",
    "    high1, low1 = np.max(w1), np.min(w1)\n",
    "    high2, low2 = np.max(w2), np.min(w2)\n",
    "    high3, low3 = np.max(w3), np.min(w3)\n",
    "    high4, low4 = np.max(w4), np.min(w4)\n",
    "\n",
    "    high = max(high1, high2, high3, high4)\n",
    "    low = min(low1, low2, low3, low4)\n",
    "    \n",
    "    losses = [loss1, loss2, loss3, loss4]\n",
    "    print(\"loss1\", loss1, \"loss2\", loss2, \"loss3\", loss3, \"loss4\", loss4)\n",
    "    \n",
    "    return high, low, w1, w2, w3, w4, losses\n",
    "\n",
    "\n",
    "cd_high, cd_low, cd_w1, cd_w2, cd_w3, cd_w4, cd_losses = get_point_weights(tgt, l1, l2, l3, l4, loss_func=\"uniform\")\n",
    "uniform_high, uniform_low, uniform_w1, uniform_w2, uniform_w3, uniform_w4, uniform_losses = get_point_weights(tgt, l1, l2, l3, l4, loss_func=\"uniform\")\n",
    "\n",
    "high, low = max(cd_high, uniform_high), min(cd_low, uniform_low)\n",
    "\n",
    "#colours = visualise_point_loss(uniform_w1, high, low, 'plasma_r')\n",
    "#colours = visualise_point_loss(uniform_w4, uniform_high, uniform_low, 'plasma_r')\n",
    "colours = visualise_point_loss(cd_w2, cd_high, cd_low, 'plasma_r')\n",
    "\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "#print(colours, cloud.shape)\n",
    "point_cloud.points = o3d.utility.Vector3dVector(l1)\n",
    "point_cloud.colors = o3d.utility.Vector3dVector(colours)\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = o3d.geometry.PointCloud()\n",
    "#print(colours, cloud.shape)\n",
    "point_cloud.points = o3d.utility.Vector3dVector(tgt)\n",
    "point_cloud.paint_uniform_color([0.1, 0.8, 0.6])\n",
    "\n",
    "o3d.visualization.draw_geometries([point_cloud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(normalize([uniform_losses]))\n",
    "X = np.arange(4)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "#ax.bar(X + 0.00, normalize([uniform_losses])[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, normalize([cd_losses])[0], color = 'mediumslateblue', width = 0.8)\n",
    "ax.set(ylabel=\"Loss (normalised)\")\n",
    "ax.set(title=\"Chamfer distance\")\n",
    "\n",
    "x1,x2,y1,y2 = plt.axis()  \n",
    "plt.axis((x1,x2,0,0.7))\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rc('xtick', labelsize=10) \n",
    "#ax.bar(X + 0.00, normalize([uniform_losses])[0], color = 'b', width = 0.25)\n",
    "ax.bar(X + 0.25, normalize([uniform_losses])[0], color = 'indianred', width = 0.8)\n",
    "ax.set(ylabel=\"Loss (normalised)\")\n",
    "ax.set(title=\"UniformCD distance\")\n",
    "x1,x2,y1,y2 = plt.axis()  \n",
    "plt.axis((x1,x2,0,0.7))\n",
    "ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CD EMD scatterplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate scatterplot to show correlations between EMD and Chamfer metrics for completion testset. CD, EMD values must saved as two lists in the 'out_metrics_<loss>.pkl file.\n",
    "dataset = \"mvp\"\n",
    "\n",
    "if dataset == \"mvp\":\n",
    "    chamfer_path = \"Density_aware_Chamfer_Distance/out_metrics_cd.pkl\" #MVP \n",
    "    uniform_path = \"Density_aware_Chamfer_Distance/out_metrics_uniform.pkl\" #MVP \n",
    "else:\n",
    "    chamfer_path = \"data/out_metrics_cd.pkl\" #PCN\n",
    "    uniform_path = \"data/out_metrics.pkl\" #PCN\n",
    "\n",
    "with open (chamfer_path, \"rb\") as f:\n",
    "    chamfer_cd, chamfer_emd = pickle.load(f)\n",
    "    \n",
    "if dataset == \"mvp\":\n",
    "    \n",
    "    chamfer_cd, chamfer_emd = [x.detach().cpu().numpy() for x in chamfer_cd], [x.detach().cpu().numpy() for x in chamfer_emd]\n",
    "    chamfer_cd, chamfer_emd = np.array(chamfer_cd).flatten(), np.array(chamfer_emd).flatten()\n",
    "    chamfer_cd = chamfer_cd*1000\n",
    "    chamfer_emd = chamfer_emd*100\n",
    "else:\n",
    "    chamfer_cd = np.array(chamfer_cd)\n",
    "    chamfer_emd = np.array(chamfer_emd)*100\n",
    "\n",
    "with open (uniform_path, \"rb\") as f:\n",
    "    uniform_cd, uniform_emd = pickle.load(f)\n",
    "    \n",
    "if dataset == \"mvp\":\n",
    "    uniform_cd, uniform_emd = [x.detach().cpu().numpy() for x in uniform_cd], [x.detach().cpu().numpy() for x in uniform_emd]\n",
    "    uniform_cd, uniform_emd = np.array(uniform_cd).flatten(), np.array(uniform_emd).flatten()\n",
    "    uniform_cd = uniform_cd*1000\n",
    "    uniform_emd = uniform_emd*100\n",
    "else:\n",
    "    uniform_cd = np.array(uniform_cd)\n",
    "    uniform_emd = np.array(uniform_emd)*100\n",
    "    \n",
    "    \n",
    "print(np.average(uniform_cd), np.average(chamfer_cd), np.average(uniform_emd), np.average(chamfer_emd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams['figure.figsize']=(10,6)\n",
    "plt.xlabel('CD (x$10^4$)', fontsize=20)\n",
    "plt.ylabel('EMD (x$10^2$)', fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.title('MVP Testset Correlation between CD and EMD, VRC model', fontsize=20)\n",
    "\n",
    "plt.scatter(uniform_cd, uniform_emd, s=3, color=\"lightseagreen\", label=\"UniformCD loss\")\n",
    "plt.scatter(chamfer_cd, chamfer_emd, s=3, color=\"palevioletred\", label=\"CD loss\")\n",
    "plt.legend(prop = { \"size\": 18 })\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find correlation between chamfer and EMD\n",
    "\n",
    "print(\"uniform r\", scipy.stats.pearsonr(uniform_cd, uniform_emd))\n",
    "print(\"chamfer r\", scipy.stats.pearsonr(chamfer_cd, chamfer_emd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise loss curves for training VCN models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train log and get eval records\n",
    "train_log_file = \"../experiments/Density_aware_Chamfer_Distance/log/vrcnet_plus_cd_debug_2024-03-11T17:00:03/train - Copy.log\"\n",
    "def get_metrics_from_log(train_log_file, uniform=False):\n",
    "    with open(train_log_file, \"rb\") as f:\n",
    "        train_log = f.readlines()\n",
    "        \n",
    "    eval_lines = []\n",
    "    for line in train_log:\n",
    "        if \"curr\" in str(line):\n",
    "            eval_lines.append(line)\n",
    "            \n",
    "            \n",
    "    # get results\n",
    "    dcd, cd, emd, bcd = [], [] ,[], []\n",
    "\n",
    "    for line in eval_lines:\n",
    "        line = str(line)\n",
    "        dcd.append(float(line.split(\"dcd: \")[1].split(\";\")[0]))\n",
    "        cd.append(float(line.split(\"cd_t: \")[1].split(\";\")[0]))\n",
    "        emd.append(float(line.split(\"emd: \")[1].split(\";\")[0]))\n",
    "        if uniform:\n",
    "            bcd.append(float(line.split(\"dcd: \")[1].split(\";\")[0]))\n",
    "\n",
    "    \n",
    "    return dcd, cd, emd, bcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_file = \"../experiments/Density_aware_Chamfer_Distance/log/vrcnet_plus_cd_debug_2024-03-11T17:00:03/train - Copy.log\"\n",
    "dcd_cd, cd_cd, emd_cd, _ = get_metrics_from_log(train_log_file)\n",
    "\n",
    "train_log_file = \"../experiments/Density_aware_Chamfer_Distance/log/vrcnet_plus_cd_debug_2024-03-11T17:00:03/train - Copy.log\"\n",
    "dcd_u, cd_u, emd_u, bcd_u = get_metrics_from_log(train_log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise losses\n",
    "iterations = list(range(2, len(cd_cd)*2+2, 2))\n",
    "\n",
    "# Plotting the line graph\n",
    "#plt.plot(iterations, dcd_cd, label='DCD')\n",
    "#plt.plot(iterations, emd_cd, label='EMD')\n",
    "plt.plot(iterations, cd_cd, label='CD')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evaluation')\n",
    "\n",
    "# Displaying the graph\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
